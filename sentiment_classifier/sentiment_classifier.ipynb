{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size: 211225\n",
      "                                                text       id  \\\n",
      "0                                    That game hurt.  eew5j0j   \n",
      "1   >sexuality shouldn’t be a grouping category I...  eemcysk   \n",
      "2     You do right, if you don't care then fuck 'em!  ed2mah1   \n",
      "3                                 Man I love reddit.  eeibobj   \n",
      "4  [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
      "\n",
      "                author            subreddit    link_id   parent_id  \\\n",
      "0                Brdd9                  nrl  t3_ajis4z  t1_eew18eq   \n",
      "1          TheGreen888     unpopularopinion  t3_ai4q37   t3_ai4q37   \n",
      "2             Labalool          confessions  t3_abru74  t1_ed2m7g7   \n",
      "3        MrsRobertshaw             facepalm  t3_ahulml   t3_ahulml   \n",
      "4  American_Fascist713  starwarsspeculation  t3_ackt2f  t1_eda65q2   \n",
      "\n",
      "    created_utc  rater_id  example_very_unclear  admiration  ...  love  \\\n",
      "0  1.548381e+09         1                 False           0  ...     0   \n",
      "1  1.548084e+09        37                  True           0  ...     0   \n",
      "2  1.546428e+09        37                 False           0  ...     0   \n",
      "3  1.547965e+09        18                 False           0  ...     1   \n",
      "4  1.546669e+09         2                 False           0  ...     0   \n",
      "\n",
      "   nervousness  optimism  pride  realization  relief  remorse  sadness  \\\n",
      "0            0         0      0            0       0        0        1   \n",
      "1            0         0      0            0       0        0        0   \n",
      "2            0         0      0            0       0        0        0   \n",
      "3            0         0      0            0       0        0        0   \n",
      "4            0         0      0            0       0        0        0   \n",
      "\n",
      "   surprise  neutral  \n",
      "0         0        0  \n",
      "1         0        0  \n",
      "2         0        1  \n",
      "3         0        0  \n",
      "4         0        1  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DefaultDataCollator\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, Value, Sequence\n",
    "\n",
    "# 1. Read CSV files\n",
    "df1 = pd.read_csv(\"goemotions/data/full_dataset/goemotions_1.csv\")\n",
    "df2 = pd.read_csv(\"goemotions/data/full_dataset/goemotions_2.csv\")\n",
    "df3 = pd.read_csv(\"goemotions/data/full_dataset/goemotions_3.csv\")\n",
    "\n",
    "# Combine them if needed (this step depends on how your CSVs are structured)\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Inspect\n",
    "print(\"Combined dataset size:\", len(df))\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New label set: ['admiration', 'anger', 'approval', 'confusion', 'curiosity', 'desire', 'disapproval', 'embarrassment', 'fear', 'gratitude', 'joy', 'love', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Suppose your CSV has a column \"labels\" that is a list of emotion strings:\n",
    "# e.g. row[\"labels\"] might be [\"joy\", \"amusement\"]\n",
    "\n",
    "label_groups = {\n",
    "    \"anger\": \"anger\",\n",
    "    \"annoyance\": \"anger\",\n",
    "    \"disgust\": \"anger\",\n",
    "    \"joy\": \"joy\",\n",
    "    \"amusement\": \"joy\",\n",
    "    \"excitement\": \"joy\",\n",
    "    \"sadness\": \"sadness\",\n",
    "    \"grief\": \"sadness\",\n",
    "    \"disappointment\": \"sadness\",\n",
    "    \"love\": \"love\",\n",
    "    \"caring\": \"love\",\n",
    "    \"fear\": \"fear\",\n",
    "    \"nervousness\": \"fear\",\n",
    "    \"surprise\": \"surprise\",\n",
    "    # ... etc ...\n",
    "    \"admiration\": \"admiration\",\n",
    "    \"approval\": \"approval\",\n",
    "    \"confusion\": \"confusion\",\n",
    "    \"curiosity\": \"curiosity\",\n",
    "    \"desire\": \"desire\",\n",
    "    \"disapproval\": \"disapproval\",\n",
    "    \"embarrassment\": \"embarrassment\",\n",
    "    \"gratitude\": \"gratitude\",\n",
    "    \"optimism\": \"optimism\",\n",
    "    \"pride\": \"pride\",\n",
    "    \"realization\": \"realization\",\n",
    "    \"relief\": \"relief\",\n",
    "    \"remorse\": \"remorse\",\n",
    "    \"neutral\": \"neutral\"\n",
    "}\n",
    "\n",
    "# Build a sorted list of unique new labels:\n",
    "unique_new_labels = sorted(list(set(label_groups.values())))\n",
    "print(\"New label set:\", unique_new_labels)\n",
    "# e.g. ['admiration', 'anger', 'approval', 'confusion', 'curiosity', 'desire', \n",
    "#       'disapproval', 'embarrassment', 'fear', 'gratitude', 'joy', 'love', \n",
    "#       'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', \n",
    "#       'sadness', 'surprise']\n",
    "\n",
    "# Create a mapping from label -> index\n",
    "new_label2id = {lbl: i for i, lbl in enumerate(unique_new_labels)}\n",
    "\n",
    "def map_labels_to_new(labels_list):\n",
    "    \"\"\"\n",
    "    labels_list is the list of original label strings for a single example.\n",
    "    We'll map each to the new label, then build a multi-hot vector.\n",
    "    \"\"\"\n",
    "    # Start all zeros\n",
    "    multi_hot = [0] * len(unique_new_labels)\n",
    "    for old_lbl in labels_list:\n",
    "        if old_lbl in label_groups:\n",
    "            new_lbl = label_groups[old_lbl]\n",
    "            idx = new_label2id[new_lbl]\n",
    "            multi_hot[idx] = 1\n",
    "        # If there's a label not in label_groups, decide how to handle it (ignore or add it)\n",
    "    return multi_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_groups = {\n",
    "    \"anger\": \"anger\",\n",
    "    \"disgust\": \"anger\",   # Merge disgust into \"anger\"\n",
    "\n",
    "    \"joy\": \"joy\",\n",
    "    \"amusement\": \"joy\",\n",
    "    \"excitement\": \"joy\",\n",
    "\n",
    "    \"sadness\": \"sadness\",\n",
    "    \"grief\": \"sadness\",\n",
    "    \"disappointment\": \"sadness\",\n",
    "\n",
    "    \"love\": \"love\",\n",
    "    \"caring\": \"love\",\n",
    "\n",
    "    \"fear\": \"fear\",\n",
    "    \"nervousness\": \"fear\",\n",
    "\n",
    "    # Keep some as-is:\n",
    "    \"admiration\": \"admiration\",\n",
    "    \"approval\":   \"approval\",\n",
    "    \"confusion\":  \"confusion\",\n",
    "    \"curiosity\":  \"curiosity\",\n",
    "    \"desire\":     \"desire\",\n",
    "    \"disapproval\":\"disapproval\",\n",
    "    \"embarrassment\":\"embarrassment\",\n",
    "    \"gratitude\":  \"gratitude\",\n",
    "    \"optimism\":   \"optimism\",\n",
    "    \"pride\":      \"pride\",\n",
    "    \"realization\":\"realization\",\n",
    "    \"relief\":     \"relief\",\n",
    "    \"remorse\":    \"remorse\",\n",
    "    \"sadness\":    \"sadness\",\n",
    "    \"surprise\":   \"surprise\",\n",
    "    \"neutral\":    \"neutral\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New label set: ['admiration', 'anger', 'approval', 'confusion', 'curiosity', 'desire', 'disapproval', 'embarrassment', 'fear', 'gratitude', 'joy', 'love', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "unique_new_labels = sorted(set(label_groups.values()))\n",
    "print(\"New label set:\", unique_new_labels)\n",
    "new_label2id = {lbl: i for i, lbl in enumerate(unique_new_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df: Index(['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id',\n",
      "       'created_utc', 'rater_id', 'example_very_unclear', 'admiration',\n",
      "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
      "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
      "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
      "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
      "       'remorse', 'sadness', 'surprise', 'neutral'],\n",
      "      dtype='object')\n",
      "Dataset size: 70000\n",
      "New label set: ['admiration', 'anger', 'approval', 'confusion', 'curiosity', 'desire', 'disapproval', 'embarrassment', 'fear', 'gratitude', 'joy', 'love', 'neutral', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DefaultDataCollator\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, Sequence, Value\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1) Read your local CSV\n",
    "# If you have multiple CSVs (goemotions_1.csv, goemotions_2.csv, etc.), concatenate them\n",
    "df = pd.read_csv(\"goemotions/data/full_dataset/goemotions_1.csv\")\n",
    "# df2 = pd.read_csv(\"goemotions/data/full_dataset/goemotions_2.csv\")\n",
    "# df3 = pd.read_csv(\"goemotions/data/full_dataset/goemotions_3.csv\")\n",
    "# df = pd.concat([df, df2, df3], ignore_index=True)\n",
    "\n",
    "print(\"Columns in df:\", df.columns)\n",
    "print(\"Dataset size:\", len(df))\n",
    "\n",
    "# 2) List of emotion columns (one column per emotion, with 0/1 values).\n",
    "#    Make sure these match EXACTLY the columns in your DataFrame.\n",
    "emotions = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"approval\", \"caring\",\n",
    "    \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\",\n",
    "    \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\",\n",
    "    \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\", \"pride\",\n",
    "    \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"\n",
    "]\n",
    "\n",
    "# 3) Define how to merge overlapping emotions:\n",
    "#    Key = original emotion column, Value = your new \"merged\" label name.\n",
    "label_groups = {\n",
    "    # Merge 'disgust' into 'anger'\n",
    "    \"anger\": \"anger\",\n",
    "    \"disgust\": \"anger\",\n",
    "\n",
    "    # Merge 'amusement' & 'excitement' into 'joy'\n",
    "    \"joy\": \"joy\",\n",
    "    \"amusement\": \"joy\",\n",
    "    \"excitement\": \"joy\",\n",
    "\n",
    "    # Merge 'grief' & 'disappointment' into 'sadness'\n",
    "    \"sadness\": \"sadness\",\n",
    "    \"grief\": \"sadness\",\n",
    "    \"disappointment\": \"sadness\",\n",
    "\n",
    "    # Merge 'caring' into 'love'\n",
    "    \"love\": \"love\",\n",
    "    \"caring\": \"love\",\n",
    "\n",
    "    # Merge 'nervousness' into 'fear'\n",
    "    \"fear\": \"fear\",\n",
    "    \"nervousness\": \"fear\",\n",
    "\n",
    "    # Keep everything else as is\n",
    "    \"admiration\": \"admiration\",\n",
    "    \"approval\": \"approval\",\n",
    "    \"confusion\": \"confusion\",\n",
    "    \"curiosity\": \"curiosity\",\n",
    "    \"desire\": \"desire\",\n",
    "    \"disapproval\": \"disapproval\",\n",
    "    \"embarrassment\": \"embarrassment\",\n",
    "    \"gratitude\": \"gratitude\",\n",
    "    \"optimism\": \"optimism\",\n",
    "    \"pride\": \"pride\",\n",
    "    \"realization\": \"realization\",\n",
    "    \"relief\": \"relief\",\n",
    "    \"remorse\": \"remorse\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"surprise\": \"surprise\"\n",
    "}\n",
    "\n",
    "# Build a sorted list of your new merged labels\n",
    "unique_new_labels = sorted(set(label_groups.values()))\n",
    "print(\"New label set:\", unique_new_labels)\n",
    "\n",
    "# Map each merged label to an index\n",
    "new_label2id = {lbl: i for i, lbl in enumerate(unique_new_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7186/105615 [03:30<43:04, 38.09it/s]"
     ]
    }
   ],
   "source": [
    "def row_to_new_multi_hot(row):\n",
    "    # multi_hot has length = number of merged labels\n",
    "    multi_hot = [0]*len(unique_new_labels)\n",
    "    # For each original emotion column\n",
    "    for emo in emotions:\n",
    "        # If this row has emo=1, we set the merged label index to 1\n",
    "        if row[emo] == 1:\n",
    "            merged_label = label_groups[emo]  # e.g. \"anger\" if emo is \"disgust\"\n",
    "            idx = new_label2id[merged_label]\n",
    "            multi_hot[idx] = 1\n",
    "    return multi_hot\n",
    "\n",
    "# Apply the function to each row\n",
    "df[\"new_multi_hot\"] = df.apply(row_to_new_multi_hot, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 56000\n",
      "Val size: 7000\n",
      "Test size: 7000\n"
     ]
    }
   ],
   "source": [
    "# 80% train, 10% val, 10% test\n",
    "df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(df_train))\n",
    "print(\"Val size:\", len(df_val))\n",
    "print(\"Test size:\", len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_val   = Dataset.from_pandas(df_val)\n",
    "ds_test  = Dataset.from_pandas(df_test)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": ds_train,\n",
    "    \"validation\": ds_val,\n",
    "    \"test\": ds_test\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Second_Sem\\csc392\\env_py310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 56000/56000 [00:02<00:00, 26149.90 examples/s]\n",
      "Map: 100%|██████████| 7000/7000 [00:00<00:00, 27635.04 examples/s]\n",
      "Map: 100%|██████████| 7000/7000 [00:00<00:00, 28176.57 examples/s]\n",
      "Casting the dataset: 100%|██████████| 56000/56000 [00:00<00:00, 501712.08 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7000/7000 [00:00<00:00, 428346.12 examples/s]\n",
      "Casting the dataset: 100%|██████████| 7000/7000 [00:00<00:00, 422204.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"  # a smaller model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tok = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=128\n",
    "    )\n",
    "    # Attach the new_multi_hot vector as labels\n",
    "    tok[\"labels\"] = examples[\"new_multi_hot\"]\n",
    "    return tok\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Cast the labels to float for multi-label classification\n",
    "encoded_dataset = encoded_dataset.cast_column(\"labels\", Sequence(Value(\"float32\")))\n",
    "encoded_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(unique_new_labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "# Compute pos_weight if desired\n",
    "def compute_pos_weight(dataset_split, n_labels):\n",
    "    counts = np.zeros(n_labels)\n",
    "    for example in dataset_split:\n",
    "        counts += np.array(example[\"labels\"])\n",
    "    total_samples = len(dataset_split)\n",
    "    # pos_weight = (N - count) / count\n",
    "    pos_weight = (total_samples - counts) / (counts + 1e-5)\n",
    "    return torch.tensor(pos_weight, dtype=torch.float32)\n",
    "\n",
    "pos_weight = compute_pos_weight(encoded_dataset[\"train\"], num_labels)\n",
    "\n",
    "# Custom Trainer to use BCEWithLogitsLoss + pos_weight\n",
    "class WeightedBCELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs[\"labels\"]\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(logits.device))\n",
    "        loss = bce(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Define a metric function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).int().numpy()\n",
    "    subset_accuracy = np.mean(np.all(preds == labels, axis=1))\n",
    "    micro_f1 = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    return {\n",
    "        \"subset_accuracy\": subset_accuracy,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"macro_f1\": macro_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Second_Sem\\csc392\\env_py310\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 106/105615 [00:05<45:59, 38.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3005, 'learning_rate': 9.46790380609733e-08, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 207/105615 [00:08<47:13, 37.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3277, 'learning_rate': 1.893580761219466e-07, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 303/105615 [00:10<47:27, 36.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3556, 'learning_rate': 2.830903238023102e-07, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 404/105615 [00:13<46:26, 37.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.335, 'learning_rate': 3.7776936186328347e-07, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 506/105615 [00:16<45:56, 38.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3529, 'learning_rate': 4.724483999242568e-07, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 607/105615 [00:18<44:47, 39.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3172, 'learning_rate': 5.671274379852301e-07, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 705/105615 [00:21<45:40, 38.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3012, 'learning_rate': 6.618064760462034e-07, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 807/105615 [00:23<44:46, 39.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3514, 'learning_rate': 7.564855141071767e-07, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 905/105615 [00:26<45:50, 38.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2958, 'learning_rate': 8.511645521681501e-07, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1003/105615 [00:29<48:11, 36.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.336, 'learning_rate': 9.458435902291234e-07, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1104/105615 [00:31<46:15, 37.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.302, 'learning_rate': 1.0405226282900967e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1203/105615 [00:34<45:21, 38.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3126, 'learning_rate': 1.13520166635107e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1303/105615 [00:36<48:54, 35.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3054, 'learning_rate': 1.2298807044120432e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1407/105615 [00:39<45:08, 38.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3622, 'learning_rate': 1.3245597424730167e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1504/105615 [00:42<47:06, 36.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3093, 'learning_rate': 1.41829199015338e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1605/105615 [00:45<48:18, 35.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3078, 'learning_rate': 1.5129710282143534e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1705/105615 [00:47<50:08, 34.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3523, 'learning_rate': 1.6076500662753266e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1805/105615 [00:50<50:18, 34.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2882, 'learning_rate': 1.7013823139556904e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1905/105615 [00:53<49:02, 35.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2358, 'learning_rate': 1.7960613520166637e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2005/105615 [00:56<48:17, 35.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3135, 'learning_rate': 1.8907403900776369e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2105/105615 [00:59<46:10, 37.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.255, 'learning_rate': 1.9854194281386104e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2206/105615 [01:01<47:23, 36.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2553, 'learning_rate': 2.0800984661995833e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2305/105615 [01:04<46:18, 37.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2514, 'learning_rate': 2.1747775042605567e-06, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2405/105615 [01:07<46:47, 36.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.226, 'learning_rate': 2.26945654232153e-06, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2505/105615 [01:10<47:56, 35.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2778, 'learning_rate': 2.364135580382504e-06, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2605/105615 [01:12<50:54, 33.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2624, 'learning_rate': 2.4588146184434768e-06, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2705/105615 [01:15<43:17, 39.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2179, 'learning_rate': 2.55349365650445e-06, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2805/105615 [01:18<46:08, 37.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2137, 'learning_rate': 2.6481726945654235e-06, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2905/105615 [01:21<49:41, 34.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2482, 'learning_rate': 2.7428517326263964e-06, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3005/105615 [01:24<47:29, 36.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2208, 'learning_rate': 2.83753077068737e-06, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3106/105615 [01:26<48:43, 35.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2908, 'learning_rate': 2.9322098087483435e-06, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3205/105615 [01:29<45:38, 37.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1706, 'learning_rate': 3.0268888468093164e-06, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3304/105615 [01:32<45:51, 37.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1336, 'learning_rate': 3.1215678848702902e-06, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3404/105615 [01:34<47:12, 36.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0896, 'learning_rate': 3.216246922931263e-06, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3506/105615 [01:37<45:26, 37.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1419, 'learning_rate': 3.310925960992237e-06, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3603/105615 [01:40<45:38, 37.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0961, 'learning_rate': 3.40560499905321e-06, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3705/105615 [01:42<46:19, 36.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2392, 'learning_rate': 3.500284037114183e-06, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3805/105615 [01:45<43:29, 39.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1702, 'learning_rate': 3.594963075175156e-06, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3905/105615 [01:48<44:56, 37.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0587, 'learning_rate': 3.68964211323613e-06, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4003/105615 [01:50<47:32, 35.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2097, 'learning_rate': 3.784321151297103e-06, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4107/105615 [01:53<45:28, 37.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9737, 'learning_rate': 3.879000189358077e-06, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4203/105615 [01:56<46:43, 36.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.127, 'learning_rate': 3.97367922741905e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4305/105615 [01:58<44:08, 38.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1101, 'learning_rate': 4.068358265480023e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4405/105615 [02:01<44:57, 37.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1097, 'learning_rate': 4.163037303540997e-06, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4506/105615 [02:04<45:43, 36.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0491, 'learning_rate': 4.257716341601969e-06, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4604/105615 [02:06<43:11, 38.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1313, 'learning_rate': 4.352395379662943e-06, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4706/105615 [02:09<44:06, 38.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1074, 'learning_rate': 4.446127627343307e-06, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4803/105615 [02:12<45:03, 37.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1194, 'learning_rate': 4.54080666540428e-06, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4905/105615 [02:14<46:41, 35.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1655, 'learning_rate': 4.6354857034652534e-06, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 5005/105615 [02:17<47:08, 35.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0931, 'learning_rate': 4.730164741526227e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 5106/105615 [02:20<44:11, 37.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0871, 'learning_rate': 4.8248437795872e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 5208/105615 [02:23<44:59, 37.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0289, 'learning_rate': 4.919522817648173e-06, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5306/105615 [02:25<47:01, 35.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0624, 'learning_rate': 5.014201855709147e-06, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5403/105615 [02:28<46:31, 35.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9564, 'learning_rate': 5.108880893770119e-06, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5508/105615 [02:31<43:36, 38.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0435, 'learning_rate': 5.203559931831093e-06, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5604/105615 [02:34<44:18, 37.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0715, 'learning_rate': 5.298238969892067e-06, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5706/105615 [02:37<46:45, 35.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0944, 'learning_rate': 5.392918007953039e-06, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5803/105615 [02:39<44:21, 37.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9906, 'learning_rate': 5.487597046014013e-06, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5906/105615 [02:42<47:16, 35.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0333, 'learning_rate': 5.582276084074987e-06, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6006/105615 [02:45<46:12, 35.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0273, 'learning_rate': 5.6769551221359594e-06, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6106/105615 [02:48<44:43, 37.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0863, 'learning_rate': 5.771634160196933e-06, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6206/105615 [02:50<44:20, 37.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0505, 'learning_rate': 5.866313198257906e-06, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6306/105615 [02:53<45:41, 36.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9142, 'learning_rate': 5.9609922363188795e-06, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6406/105615 [02:56<43:41, 37.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0958, 'learning_rate': 6.055671274379853e-06, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6506/105615 [02:58<43:29, 37.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.935, 'learning_rate': 6.150350312440826e-06, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6606/105615 [03:01<46:12, 35.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0416, 'learning_rate': 6.245029350501799e-06, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6706/105615 [03:04<43:28, 37.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0358, 'learning_rate': 6.339708388562773e-06, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6806/105615 [03:06<44:26, 37.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9607, 'learning_rate': 6.434387426623746e-06, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6906/105615 [03:09<44:51, 36.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9704, 'learning_rate': 6.529066464684719e-06, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7003/105615 [03:12<43:57, 37.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1562, 'learning_rate': 6.622798712365083e-06, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7103/105615 [03:14<44:07, 37.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.047, 'learning_rate': 6.7174777504260555e-06, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7183/105615 [03:17<43:04, 38.09it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 27\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./distilbert_merged_emotions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# if your GPU supports it\u001b[39;00m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m WeightedBCELossTrainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Second_Sem\\csc392\\env_py310\\lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Second_Sem\\csc392\\env_py310\\lib\\site-packages\\transformers\\trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1808\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1809\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1812\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1814\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1815\u001b[0m ):\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1817\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_merged_emotions\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    fp16=True  # if your GPU supports it\n",
    ")\n",
    "\n",
    "trainer = WeightedBCELossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained embeddings...\n",
      "Found 781 pretrained embeddings out of 999 words\n",
      "Epoch 1/100:\n",
      "Train Loss: 0.7594, Accuracy: 0.5036, F1: 0.4567\n",
      "Val Loss: 0.7008, Accuracy: 0.4384, F1: 0.6095\n",
      "Val Precision: 0.4384, Recall: 1.0000\n",
      "\n",
      "Epoch 2/100:\n",
      "Train Loss: 0.7014, Accuracy: 0.5396, F1: 0.4921\n",
      "Val Loss: 0.7107, Accuracy: 0.4384, F1: 0.6095\n",
      "Val Precision: 0.4384, Recall: 1.0000\n",
      "\n",
      "Epoch 3/100:\n",
      "Train Loss: 0.6472, Accuracy: 0.6043, F1: 0.5669\n",
      "Val Loss: 0.7164, Accuracy: 0.4384, F1: 0.6095\n",
      "Val Precision: 0.4384, Recall: 1.0000\n",
      "\n",
      "Epoch 00004: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 4/100:\n",
      "Train Loss: 0.6401, Accuracy: 0.6403, F1: 0.6154\n",
      "Val Loss: 0.7257, Accuracy: 0.4384, F1: 0.6095\n",
      "Val Precision: 0.4384, Recall: 1.0000\n",
      "\n",
      "Epoch 5/100:\n",
      "Train Loss: 0.6120, Accuracy: 0.6331, F1: 0.6331\n",
      "Val Loss: 0.7173, Accuracy: 0.4384, F1: 0.6095\n",
      "Val Precision: 0.4384, Recall: 1.0000\n",
      "\n",
      "Epoch 6/100:\n",
      "Train Loss: 0.6914, Accuracy: 0.6187, F1: 0.6015\n",
      "Val Loss: 0.7184, Accuracy: 0.4384, F1: 0.6095\n",
      "Val Precision: 0.4384, Recall: 1.0000\n",
      "\n",
      "Epoch 00007: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 7/100:\n",
      "Train Loss: 0.6005, Accuracy: 0.6619, F1: 0.6619\n",
      "Val Loss: 0.7063, Accuracy: 0.4384, F1: 0.6095\n",
      "Val Precision: 0.4384, Recall: 1.0000\n",
      "\n",
      "Epoch 8/100:\n",
      "Train Loss: 0.5607, Accuracy: 0.6906, F1: 0.6950\n",
      "Val Loss: 0.7064, Accuracy: 0.4384, F1: 0.6095\n",
      "Val Precision: 0.4384, Recall: 1.0000\n",
      "\n",
      "Epoch 9/100:\n",
      "Train Loss: 0.6014, Accuracy: 0.6475, F1: 0.6423\n",
      "Val Loss: 0.7083, Accuracy: 0.4521, F1: 0.6078\n",
      "Val Precision: 0.4429, Recall: 0.9688\n",
      "\n",
      "Epoch 10/100:\n",
      "Train Loss: 0.5139, Accuracy: 0.7698, F1: 0.7612\n",
      "Val Loss: 0.6963, Accuracy: 0.4795, F1: 0.5778\n",
      "Val Precision: 0.4483, Recall: 0.8125\n",
      "\n",
      "Epoch 11/100:\n",
      "Train Loss: 0.5501, Accuracy: 0.7338, F1: 0.7448\n",
      "Val Loss: 0.6881, Accuracy: 0.5342, F1: 0.6047\n",
      "Val Precision: 0.4815, Recall: 0.8125\n",
      "\n",
      "Epoch 12/100:\n",
      "Train Loss: 0.5600, Accuracy: 0.7194, F1: 0.7194\n",
      "Val Loss: 0.6858, Accuracy: 0.5342, F1: 0.6047\n",
      "Val Precision: 0.4815, Recall: 0.8125\n",
      "\n",
      "Epoch 13/100:\n",
      "Train Loss: 0.5321, Accuracy: 0.7554, F1: 0.7500\n",
      "Val Loss: 0.6867, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 14/100:\n",
      "Train Loss: 0.5134, Accuracy: 0.7698, F1: 0.7714\n",
      "Val Loss: 0.6845, Accuracy: 0.5753, F1: 0.6173\n",
      "Val Precision: 0.5102, Recall: 0.7812\n",
      "\n",
      "Epoch 15/100:\n",
      "Train Loss: 0.5135, Accuracy: 0.7410, F1: 0.7273\n",
      "Val Loss: 0.6843, Accuracy: 0.5753, F1: 0.6076\n",
      "Val Precision: 0.5106, Recall: 0.7500\n",
      "\n",
      "Epoch 16/100:\n",
      "Train Loss: 0.5492, Accuracy: 0.6978, F1: 0.6912\n",
      "Val Loss: 0.6897, Accuracy: 0.5753, F1: 0.6076\n",
      "Val Precision: 0.5106, Recall: 0.7500\n",
      "\n",
      "Epoch 17/100:\n",
      "Train Loss: 0.4787, Accuracy: 0.8058, F1: 0.8058\n",
      "Val Loss: 0.6882, Accuracy: 0.5753, F1: 0.6076\n",
      "Val Precision: 0.5106, Recall: 0.7500\n",
      "\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 18/100:\n",
      "Train Loss: 0.4950, Accuracy: 0.7626, F1: 0.7556\n",
      "Val Loss: 0.6977, Accuracy: 0.5890, F1: 0.6154\n",
      "Val Precision: 0.5217, Recall: 0.7500\n",
      "\n",
      "Epoch 19/100:\n",
      "Train Loss: 0.5109, Accuracy: 0.7266, F1: 0.7324\n",
      "Val Loss: 0.7024, Accuracy: 0.5753, F1: 0.6076\n",
      "Val Precision: 0.5106, Recall: 0.7500\n",
      "\n",
      "Epoch 20/100:\n",
      "Train Loss: 0.5360, Accuracy: 0.7338, F1: 0.7413\n",
      "Val Loss: 0.7033, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 00021: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 21/100:\n",
      "Train Loss: 0.4988, Accuracy: 0.7626, F1: 0.7692\n",
      "Val Loss: 0.7037, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 22/100:\n",
      "Train Loss: 0.4893, Accuracy: 0.7770, F1: 0.7737\n",
      "Val Loss: 0.7030, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 23/100:\n",
      "Train Loss: 0.5173, Accuracy: 0.7266, F1: 0.7206\n",
      "Val Loss: 0.7054, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 00024: reducing learning rate of group 0 to 3.1250e-06.\n",
      "Epoch 24/100:\n",
      "Train Loss: 0.4980, Accuracy: 0.7842, F1: 0.7857\n",
      "Val Loss: 0.7016, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 25/100:\n",
      "Train Loss: 0.4952, Accuracy: 0.7986, F1: 0.8028\n",
      "Val Loss: 0.7062, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 26/100:\n",
      "Train Loss: 0.4977, Accuracy: 0.8129, F1: 0.8060\n",
      "Val Loss: 0.7045, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.5625e-06.\n",
      "Epoch 27/100:\n",
      "Train Loss: 0.4610, Accuracy: 0.7986, F1: 0.8028\n",
      "Val Loss: 0.7127, Accuracy: 0.5342, F1: 0.5952\n",
      "Val Precision: 0.4808, Recall: 0.7812\n",
      "\n",
      "Epoch 28/100:\n",
      "Train Loss: 0.4648, Accuracy: 0.8129, F1: 0.8030\n",
      "Val Loss: 0.7095, Accuracy: 0.5479, F1: 0.6024\n",
      "Val Precision: 0.4902, Recall: 0.7812\n",
      "\n",
      "Epoch 29/100:\n",
      "Train Loss: 0.5276, Accuracy: 0.7482, F1: 0.7407\n",
      "Val Loss: 0.7025, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 00030: reducing learning rate of group 0 to 7.8125e-07.\n",
      "Epoch 30/100:\n",
      "Train Loss: 0.5348, Accuracy: 0.7266, F1: 0.7121\n",
      "Val Loss: 0.6980, Accuracy: 0.5616, F1: 0.6000\n",
      "Val Precision: 0.5000, Recall: 0.7500\n",
      "\n",
      "Epoch 31/100:\n",
      "Train Loss: 0.4721, Accuracy: 0.8201, F1: 0.8201\n",
      "Val Loss: 0.7002, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 32/100:\n",
      "Train Loss: 0.5319, Accuracy: 0.7194, F1: 0.7194\n",
      "Val Loss: 0.7064, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 00033: reducing learning rate of group 0 to 3.9063e-07.\n",
      "Epoch 33/100:\n",
      "Train Loss: 0.4760, Accuracy: 0.7770, F1: 0.7832\n",
      "Val Loss: 0.7089, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 34/100:\n",
      "Train Loss: 0.4495, Accuracy: 0.8417, F1: 0.8382\n",
      "Val Loss: 0.7095, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 35/100:\n",
      "Train Loss: 0.5455, Accuracy: 0.7050, F1: 0.7050\n",
      "Val Loss: 0.7040, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 00036: reducing learning rate of group 0 to 1.9531e-07.\n",
      "Epoch 36/100:\n",
      "Train Loss: 0.5258, Accuracy: 0.7554, F1: 0.7500\n",
      "Val Loss: 0.7077, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 37/100:\n",
      "Train Loss: 0.4968, Accuracy: 0.7266, F1: 0.7286\n",
      "Val Loss: 0.7101, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 38/100:\n",
      "Train Loss: 0.4908, Accuracy: 0.7914, F1: 0.7852\n",
      "Val Loss: 0.7029, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 00039: reducing learning rate of group 0 to 9.7656e-08.\n",
      "Epoch 39/100:\n",
      "Train Loss: 0.4759, Accuracy: 0.8058, F1: 0.7970\n",
      "Val Loss: 0.7097, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 40/100:\n",
      "Train Loss: 0.5133, Accuracy: 0.7194, F1: 0.7194\n",
      "Val Loss: 0.7104, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 41/100:\n",
      "Train Loss: 0.4797, Accuracy: 0.7842, F1: 0.7761\n",
      "Val Loss: 0.7075, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 00042: reducing learning rate of group 0 to 4.8828e-08.\n",
      "Epoch 42/100:\n",
      "Train Loss: 0.5382, Accuracy: 0.7194, F1: 0.7153\n",
      "Val Loss: 0.7066, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 43/100:\n",
      "Train Loss: 0.4911, Accuracy: 0.7842, F1: 0.7826\n",
      "Val Loss: 0.7106, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 44/100:\n",
      "Train Loss: 0.4757, Accuracy: 0.7626, F1: 0.7755\n",
      "Val Loss: 0.7074, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 00045: reducing learning rate of group 0 to 2.4414e-08.\n",
      "Epoch 45/100:\n",
      "Train Loss: 0.4943, Accuracy: 0.7770, F1: 0.7737\n",
      "Val Loss: 0.7096, Accuracy: 0.5479, F1: 0.6024\n",
      "Val Precision: 0.4902, Recall: 0.7812\n",
      "\n",
      "Epoch 46/100:\n",
      "Train Loss: 0.4609, Accuracy: 0.7626, F1: 0.7660\n",
      "Val Loss: 0.7126, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 47/100:\n",
      "Train Loss: 0.4824, Accuracy: 0.7914, F1: 0.7883\n",
      "Val Loss: 0.7063, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 00048: reducing learning rate of group 0 to 1.2207e-08.\n",
      "Epoch 48/100:\n",
      "Train Loss: 0.4910, Accuracy: 0.7554, F1: 0.7536\n",
      "Val Loss: 0.7071, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 49/100:\n",
      "Train Loss: 0.4900, Accuracy: 0.7770, F1: 0.7737\n",
      "Val Loss: 0.7102, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 50/100:\n",
      "Train Loss: 0.5047, Accuracy: 0.7626, F1: 0.7626\n",
      "Val Loss: 0.7081, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 51/100:\n",
      "Train Loss: 0.4753, Accuracy: 0.7770, F1: 0.7737\n",
      "Val Loss: 0.7110, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 52/100:\n",
      "Train Loss: 0.5336, Accuracy: 0.7338, F1: 0.7218\n",
      "Val Loss: 0.7043, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 53/100:\n",
      "Train Loss: 0.5165, Accuracy: 0.7482, F1: 0.7445\n",
      "Val Loss: 0.7097, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 54/100:\n",
      "Train Loss: 0.4931, Accuracy: 0.7698, F1: 0.7647\n",
      "Val Loss: 0.7030, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 55/100:\n",
      "Train Loss: 0.4956, Accuracy: 0.7626, F1: 0.7442\n",
      "Val Loss: 0.7094, Accuracy: 0.5479, F1: 0.6024\n",
      "Val Precision: 0.4902, Recall: 0.7812\n",
      "\n",
      "Epoch 56/100:\n",
      "Train Loss: 0.4862, Accuracy: 0.8058, F1: 0.8000\n",
      "Val Loss: 0.7100, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 57/100:\n",
      "Train Loss: 0.5236, Accuracy: 0.7338, F1: 0.7299\n",
      "Val Loss: 0.7150, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 58/100:\n",
      "Train Loss: 0.4886, Accuracy: 0.7914, F1: 0.7852\n",
      "Val Loss: 0.7114, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 59/100:\n",
      "Train Loss: 0.4762, Accuracy: 0.7914, F1: 0.7914\n",
      "Val Loss: 0.7202, Accuracy: 0.5479, F1: 0.6024\n",
      "Val Precision: 0.4902, Recall: 0.7812\n",
      "\n",
      "Epoch 60/100:\n",
      "Train Loss: 0.5004, Accuracy: 0.7842, F1: 0.7794\n",
      "Val Loss: 0.7118, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 61/100:\n",
      "Train Loss: 0.5020, Accuracy: 0.7914, F1: 0.7852\n",
      "Val Loss: 0.7093, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 62/100:\n",
      "Train Loss: 0.5425, Accuracy: 0.7770, F1: 0.7737\n",
      "Val Loss: 0.7077, Accuracy: 0.5753, F1: 0.6076\n",
      "Val Precision: 0.5106, Recall: 0.7500\n",
      "\n",
      "Epoch 63/100:\n",
      "Train Loss: 0.5431, Accuracy: 0.7626, F1: 0.7556\n",
      "Val Loss: 0.7102, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 64/100:\n",
      "Train Loss: 0.4959, Accuracy: 0.7626, F1: 0.7591\n",
      "Val Loss: 0.7146, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 65/100:\n",
      "Train Loss: 0.4762, Accuracy: 0.7914, F1: 0.7914\n",
      "Val Loss: 0.7100, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 66/100:\n",
      "Train Loss: 0.5086, Accuracy: 0.7842, F1: 0.7794\n",
      "Val Loss: 0.7141, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 67/100:\n",
      "Train Loss: 0.4818, Accuracy: 0.7698, F1: 0.7576\n",
      "Val Loss: 0.7124, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 68/100:\n",
      "Train Loss: 0.5259, Accuracy: 0.7194, F1: 0.7234\n",
      "Val Loss: 0.7116, Accuracy: 0.5342, F1: 0.5854\n",
      "Val Precision: 0.4800, Recall: 0.7500\n",
      "\n",
      "Epoch 69/100:\n",
      "Train Loss: 0.4645, Accuracy: 0.7914, F1: 0.7852\n",
      "Val Loss: 0.7091, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 70/100:\n",
      "Train Loss: 0.4893, Accuracy: 0.7482, F1: 0.7445\n",
      "Val Loss: 0.7077, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 71/100:\n",
      "Train Loss: 0.4681, Accuracy: 0.7842, F1: 0.7692\n",
      "Val Loss: 0.7063, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 72/100:\n",
      "Train Loss: 0.4774, Accuracy: 0.7986, F1: 0.7879\n",
      "Val Loss: 0.7100, Accuracy: 0.5342, F1: 0.5952\n",
      "Val Precision: 0.4808, Recall: 0.7812\n",
      "\n",
      "Epoch 73/100:\n",
      "Train Loss: 0.5047, Accuracy: 0.7842, F1: 0.7826\n",
      "Val Loss: 0.7119, Accuracy: 0.5479, F1: 0.6024\n",
      "Val Precision: 0.4902, Recall: 0.7812\n",
      "\n",
      "Epoch 74/100:\n",
      "Train Loss: 0.5130, Accuracy: 0.7554, F1: 0.7463\n",
      "Val Loss: 0.7098, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 75/100:\n",
      "Train Loss: 0.4767, Accuracy: 0.7914, F1: 0.7972\n",
      "Val Loss: 0.7124, Accuracy: 0.5479, F1: 0.6024\n",
      "Val Precision: 0.4902, Recall: 0.7812\n",
      "\n",
      "Epoch 76/100:\n",
      "Train Loss: 0.5078, Accuracy: 0.7554, F1: 0.7500\n",
      "Val Loss: 0.7107, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 77/100:\n",
      "Train Loss: 0.5009, Accuracy: 0.7410, F1: 0.7313\n",
      "Val Loss: 0.7129, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 78/100:\n",
      "Train Loss: 0.5001, Accuracy: 0.7698, F1: 0.7714\n",
      "Val Loss: 0.7144, Accuracy: 0.5479, F1: 0.5926\n",
      "Val Precision: 0.4898, Recall: 0.7500\n",
      "\n",
      "Epoch 79/100:\n",
      "Train Loss: 0.4607, Accuracy: 0.7770, F1: 0.7801\n",
      "Val Loss: 0.7146, Accuracy: 0.5479, F1: 0.6024\n",
      "Val Precision: 0.4902, Recall: 0.7812\n",
      "\n",
      "Epoch 80/100:\n",
      "Train Loss: 0.5285, Accuracy: 0.7554, F1: 0.7500\n",
      "Val Loss: 0.7086, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n",
      "Epoch 81/100:\n",
      "Train Loss: 0.5179, Accuracy: 0.7410, F1: 0.7391\n",
      "Val Loss: 0.7062, Accuracy: 0.5616, F1: 0.6098\n",
      "Val Precision: 0.5000, Recall: 0.7812\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 271\u001b[0m\n\u001b[1;32m    266\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# 8. Train the Model\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 163\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, preprocessor, config)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_texts, batch_labels \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m    162\u001b[0m     batch_texts, batch_labels \u001b[38;5;241m=\u001b[39m batch_texts\u001b[38;5;241m.\u001b[39mto(device), batch_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 163\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m    165\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 93\u001b[0m, in \u001b[0;36mImprovedTextClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     92\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dropout(embedded)\n\u001b[0;32m---> 93\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_net(lstm_out)\n\u001b[1;32m     95\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(attn_out)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Text Preprocessing Classes\n",
    "# -----------------------------\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, max_vocab_size=15000, max_seq_length=128):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.word_counts = Counter()\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        for text in texts:\n",
    "            cleaned_text = self.clean_text(text)\n",
    "            words = cleaned_text.split()\n",
    "            self.word_counts.update(words)\n",
    "        \n",
    "        # Reserve two indices for PAD and UNK tokens\n",
    "        vocab_words = [word for word, count in self.word_counts.most_common(self.max_vocab_size - 2)]\n",
    "        for word in vocab_words:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            cleaned_text = self.clean_text(text)\n",
    "            words = cleaned_text.split()\n",
    "            # Truncate or pad sequences\n",
    "            seq = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in words[:self.max_seq_length]]\n",
    "            seq = seq + [self.word2idx['<PAD>']] * (self.max_seq_length - len(seq))\n",
    "            sequences.append(seq)\n",
    "        return torch.tensor(sequences)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define the Model\n",
    "# -----------------------------\n",
    "class ImprovedTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2):\n",
    "        super(ImprovedTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embedding_dropout = nn.Dropout(0.3)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, bidirectional=True,\n",
    "                            dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "        \n",
    "    def attention_net(self, lstm_output):\n",
    "        # Compute attention weights and context vector\n",
    "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
    "        context_vector = torch.sum(attention_weights * lstm_output, dim=1)\n",
    "        return context_vector\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        attn_out = self.attention_net(lstm_out)\n",
    "        x = self.fc1(attn_out)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Function to Load Pretrained Embeddings (GloVe)\n",
    "# -----------------------------\n",
    "def load_pretrained_embeddings(embedding_path, word2idx, embed_dim):\n",
    "    print(\"Loading pretrained embeddings...\")\n",
    "    # Initialize embeddings with a uniform distribution\n",
    "    embeddings = np.random.uniform(-0.05, 0.05, (len(word2idx), embed_dim))\n",
    "    embeddings[word2idx['<PAD>']] = np.zeros(embed_dim)\n",
    "    found = 0\n",
    "    with open(embedding_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            if vector.shape[0] != embed_dim:\n",
    "                continue  # Skip if dimensions mismatch\n",
    "            if word in word2idx:\n",
    "                embeddings[word2idx[word]] = vector\n",
    "                found += 1\n",
    "    print(f\"Found {found} pretrained embeddings out of {len(word2idx)} words\")\n",
    "    return torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Training Function\n",
    "# -----------------------------\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, preprocessor, config):\n",
    "    best_f1 = 0\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch_texts, batch_labels in train_loader:\n",
    "            batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_texts)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            train_preds.extend(predictions.cpu().numpy())\n",
    "            train_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_texts, batch_labels in val_loader:\n",
    "                batch_texts, batch_labels = batch_texts.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_texts)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(predictions.cpu().numpy())\n",
    "                val_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(train_labels, train_preds, average='binary')\n",
    "        train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "        \n",
    "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_preds, average='binary')\n",
    "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.4f}, F1: {train_f1:.4f}')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}')\n",
    "        print(f'Val Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\\n')\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'preprocessor': preprocessor,\n",
    "                'config': config\n",
    "            }, 'best_model.pt')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # if patience_counter >= patience:\n",
    "        #     print(f'Early stopping triggered after epoch {epoch+1}')\n",
    "        #     break\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Configuration and Data Loading\n",
    "# -----------------------------\n",
    "config = {\n",
    "    'max_vocab_size': 15000,\n",
    "    'max_seq_length': 128,\n",
    "    'embed_dim': 300,  # Must match the dimension of the pretrained embeddings\n",
    "    'hidden_dim': 256,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.0001,\n",
    "    'num_epochs': 100,\n",
    "    'num_lstm_layers': 2,\n",
    "    'pretrained_embedding_path': '/Users/devshah/Documents/WorkSpace/University/year 3/CSC493/emphatic-AI-Winter2025/glove.6B/glove.6B.300d.txt'  # Update this path as needed\n",
    "}\n",
    "\n",
    "# Load your datasets (assumed to be in TSV format)\n",
    "train_df = pd.read_csv('train.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('test.tsv', sep='\\t')\n",
    "\n",
    "# Map your string labels to numerical values\n",
    "label_mapping = {'NOCUOUS': 0, 'INNOCUOUS': 1}\n",
    "train_df['Detected as'] = train_df['Detected as'].map(label_mapping)\n",
    "test_df['Detected as'] = test_df['Detected as'].map(label_mapping)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Preprocessing\n",
    "# -----------------------------\n",
    "preprocessor = TextPreprocessor(max_vocab_size=config['max_vocab_size'], \n",
    "                                max_seq_length=config['max_seq_length'])\n",
    "preprocessor.fit(train_df['Sentence'])\n",
    "\n",
    "X_train = preprocessor.transform(train_df['Sentence'])\n",
    "X_val = preprocessor.transform(test_df['Sentence'])\n",
    "y_train = torch.tensor(train_df['Detected as'].values)\n",
    "y_val = torch.tensor(test_df['Detected as'].values)\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "val_dataset = TextDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Model, Optimizer, Scheduler Setup\n",
    "# -----------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ImprovedTextClassifier(\n",
    "    vocab_size=len(preprocessor.word2idx),\n",
    "    embed_dim=config['embed_dim'],\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    num_classes=2,\n",
    "    num_layers=config['num_lstm_layers']\n",
    ").to(device)\n",
    "\n",
    "# Load pretrained embeddings and replace the embedding layer weights\n",
    "pretrained_weights = load_pretrained_embeddings(config['pretrained_embedding_path'], \n",
    "                                                preprocessor.word2idx, \n",
    "                                                config['embed_dim'])\n",
    "model.embedding = nn.Embedding.from_pretrained(pretrained_weights, freeze=False, padding_idx=0)\n",
    "\n",
    "# Compute class weights to help with class imbalance\n",
    "class_counts = torch.bincount(y_train)\n",
    "class_weights = 1.0 / class_counts.float()\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Train the Model\n",
    "# -----------------------------\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "            config['num_epochs'], device, preprocessor, config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

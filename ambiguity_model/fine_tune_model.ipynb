{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your existing dataset\n",
    "df = pd.read_csv('/Users/devshah/Documents/WorkSpace/University/year 3/CSC493/emphatic-AI-Winter2025/ambiguity_model/data/synthetic_data/ambiguous_prompts_dataset_distinct.csv')\n",
    "\n",
    "# Format for generative fine-tuning\n",
    "formatted_data = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    is_ambiguous = row['label'] == 1\n",
    "    \n",
    "    # Create instruction-based examples\n",
    "    instruction = \"Analyze the following prompt and determine if it is ambiguous. If it is ambiguous, explain why and suggest a clarifying question.\"\n",
    "    \n",
    "    if is_ambiguous:\n",
    "        output = \"This prompt is ambiguous. \"\n",
    "        # You can add specific reasons based on patterns in the text\n",
    "        if \"this\" in text.lower() or \"that\" in text.lower():\n",
    "            output += \"It contains demonstrative pronouns without clear referents. \"\n",
    "        elif len(text.split()) < 5:\n",
    "            output += \"It is too brief and lacks necessary context. \"\n",
    "        else:\n",
    "            output += \"It lacks specific details needed for a clear response. \"\n",
    "        \n",
    "        output += \"A good clarifying question would be: \"\n",
    "        if \"this\" in text.lower() or \"that\" in text.lower():\n",
    "            output += f\"\\\"Could you specify what you're referring to in your prompt?\\\"\"\n",
    "        elif \"how\" in text.lower():\n",
    "            output += f\"\\\"Could you provide more details about what you're trying to accomplish?\\\"\"\n",
    "        else:\n",
    "            output += f\"\\\"Could you provide more specific information about your request?\\\"\"\n",
    "    else:\n",
    "        output = \"This prompt is clear and specific. No clarification is needed.\"\n",
    "    \n",
    "    formatted_data.append({\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": text,\n",
    "        \"output\": output\n",
    "    })\n",
    "\n",
    "# Create a Hugging Face dataset\n",
    "gen_dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n",
    "\n",
    "# Split into train and validation\n",
    "gen_dataset = gen_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Enhancing dataset with features...\n",
      "Training with 130 examples, validating with 33 examples\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'AmbiguityDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 307\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 209\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, scheduler, device, fp16)\u001b[0m\n\u001b[1;32m    206\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    207\u001b[0m total_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m    210\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    211\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1035\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "\n",
    "# Download NLTK resources (only need to run once)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  # Lightweight model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "OUTPUT_DIR = \"ambiguity_model\"\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# Load and prepare the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('/Users/devshah/Documents/WorkSpace/University/year 3/CSC493/emphatic-AI-Winter2025/ambiguity_model/data/synthetic_data/ambiguous_prompts_dataset_distinct.csv')\n",
    "\n",
    "# Define ambiguity types for feature engineering\n",
    "ambiguity_types = [\n",
    "    \"referential\",    # Unclear what a pronoun or descriptor refers to\n",
    "    \"lexical\",        # Words with multiple possible meanings\n",
    "    \"syntactic\",      # Sentence structure creates multiple interpretations\n",
    "    \"scope\",          # Unclear scope of quantifiers or modifiers\n",
    "    \"underspecified\", # Missing necessary details\n",
    "    \"vague_term\"      # Using inherently vague terms (e.g., \"soon\", \"large\")\n",
    "]\n",
    "\n",
    "# Function to analyze text and identify potential ambiguities\n",
    "def analyze_ambiguity(text):\n",
    "    detected_types = []\n",
    "    ambiguous_elements = []\n",
    "    \n",
    "    # Simple rule-based detection\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Check for referential ambiguity\n",
    "    if any(word in tokens for word in [\"it\", \"this\", \"that\", \"these\", \"those\"]):\n",
    "        detected_types.append(\"referential\")\n",
    "        ambiguous_elements.append(\"unclear pronoun reference\")\n",
    "    \n",
    "    # Check for vague terms\n",
    "    vague_terms = [\"soon\", \"many\", \"few\", \"several\", \"some\", \"a lot\", \"big\", \"small\"]\n",
    "    if any(term in ' '.join(tokens) for term in vague_terms):\n",
    "        detected_types.append(\"vague_term\")\n",
    "        for term in vague_terms:\n",
    "            if term in ' '.join(tokens):\n",
    "                ambiguous_elements.append(f\"vague term: {term}\")\n",
    "    \n",
    "    # Check for underspecification\n",
    "    if len(tokens) < 7:\n",
    "        detected_types.append(\"underspecified\")\n",
    "        ambiguous_elements.append(\"too brief\")\n",
    "    \n",
    "    return detected_types, ambiguous_elements\n",
    "\n",
    "# Enhance dataset with features\n",
    "print(\"Enhancing dataset with features...\")\n",
    "features = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    is_ambiguous = row['label'] == 1\n",
    "    \n",
    "    # Analyze ambiguity\n",
    "    detected_types, ambiguous_elements = analyze_ambiguity(text)\n",
    "    \n",
    "    # Create feature dict\n",
    "    feature = {\n",
    "        \"text\": text,\n",
    "        \"is_ambiguous\": is_ambiguous,\n",
    "        \"ambiguity_types\": detected_types,\n",
    "        \"ambiguous_elements\": ambiguous_elements,\n",
    "        \"word_count\": len(text.split())\n",
    "    }\n",
    "    features.append(feature)\n",
    "\n",
    "# Create enhanced DataFrame\n",
    "enhanced_df = pd.DataFrame(features)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df, val_df = train_test_split(enhanced_df, test_size=0.2, random_state=42, stratify=enhanced_df['is_ambiguous'])\n",
    "\n",
    "print(f\"Training with {len(train_df)} examples, validating with {len(val_df)} examples\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create dataset class\n",
    "class AmbiguityDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AmbiguityDataset(\n",
    "    texts=train_df['text'].tolist(),\n",
    "    labels=train_df['is_ambiguous'].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "val_dataset = AmbiguityDataset(\n",
    "    texts=val_df['text'].tolist(),\n",
    "    labels=val_df['is_ambiguous'].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ").to(DEVICE)\n",
    "\n",
    "# Handle 16-bit precision for efficiency\n",
    "fp16 = False\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        from torch.cuda.amp import autocast\n",
    "        fp16 = True\n",
    "        print(\"Using mixed precision training\")\n",
    "    except ImportError:\n",
    "        print(\"Mixed precision training not available\")\n",
    "\n",
    "# Prepare optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device, fp16):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with or without mixed precision\n",
    "        if fp16:\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "        else:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        # Get predictions\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        total_predictions += len(labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Release memory\n",
    "        del input_ids, attention_mask, labels, outputs, loss\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return correct_predictions.double() / total_predictions, np.mean(losses)\n",
    "\n",
    "# Evaluation function\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            total_predictions += len(labels)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            all_predictions.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            \n",
    "            # Release memory\n",
    "            del input_ids, attention_mask, labels, outputs, loss\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return (\n",
    "        correct_predictions.double() / total_predictions,\n",
    "        np.mean(losses),\n",
    "        all_predictions,\n",
    "        all_labels\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    \n",
    "    # Train\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model=model,\n",
    "        data_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=DEVICE,\n",
    "        fp16=fp16\n",
    "    )\n",
    "    \n",
    "    print(f'Train loss: {train_loss} | Train accuracy: {train_acc}')\n",
    "    \n",
    "    # Evaluate\n",
    "    val_acc, val_loss, predictions, truth = eval_model(\n",
    "        model=model,\n",
    "        data_loader=val_loader,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    print(f'Val loss: {val_loss} | Val accuracy: {val_acc}')\n",
    "    print('\\n')\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_acc > best_accuracy:\n",
    "        print(f\"Saving best model with accuracy: {val_acc}\")\n",
    "        best_accuracy = val_acc\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), f'{OUTPUT_DIR}/best_model.bin')\n",
    "        # Save the tokenizer\n",
    "        tokenizer.save_pretrained(f'{OUTPUT_DIR}')\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "# Load the best model for final evaluation\n",
    "model.load_state_dict(torch.load(f'{OUTPUT_DIR}/best_model.bin'))\n",
    "\n",
    "# Final evaluation\n",
    "val_acc, val_loss, predictions, truth = eval_model(\n",
    "    model=model,\n",
    "    data_loader=val_loader,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(truth, predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(truth, predictions))\n",
    "\n",
    "# Save model and tokenizer\n",
    "print(\"Saving model...\")\n",
    "model.save_pretrained(f'{OUTPUT_DIR}/final_model')\n",
    "tokenizer.save_pretrained(f'{OUTPUT_DIR}/final_model')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Example of how to use the trained model for inference\n",
    "def predict_ambiguity(text, model, tokenizer, device):\n",
    "    # Prepare the text\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "    \n",
    "    # Get confidence scores\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    confidence = probs[0][preds[0]].item()\n",
    "    \n",
    "    # Analyze ambiguity types if predicted as ambiguous\n",
    "    is_ambiguous = bool(preds[0].item())\n",
    "    \n",
    "    if is_ambiguous:\n",
    "        detected_types, ambiguous_elements = analyze_ambiguity(text)\n",
    "        \n",
    "        # Generate clarifying question based on ambiguity type\n",
    "        clarifying_question = \"Could you please clarify your request with more specific details?\"\n",
    "        \n",
    "        if \"referential\" in detected_types:\n",
    "            clarifying_question = \"What specifically are you referring to in your request?\"\n",
    "        elif \"vague_term\" in detected_types:\n",
    "            vague_words = [elem.split(\": \")[1] for elem in ambiguous_elements if \"vague term\" in elem]\n",
    "            if vague_words:\n",
    "                clarifying_question = f\"Could you be more specific about what you mean by '{vague_words[0]}'?\"\n",
    "        elif \"underspecified\" in detected_types:\n",
    "            clarifying_question = \"Could you provide more details about what you're trying to accomplish?\"\n",
    "        \n",
    "        return {\n",
    "            \"is_ambiguous\": True,\n",
    "            \"confidence\": confidence,\n",
    "            \"ambiguity_types\": detected_types,\n",
    "            \"ambiguous_elements\": ambiguous_elements,\n",
    "            \"clarifying_question\": clarifying_question\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"is_ambiguous\": False,\n",
    "            \"confidence\": confidence\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nTesting the model with example prompts:\")\n",
    "test_prompts = [\n",
    "    \"Write a summary of this article.\",\n",
    "    \"Create a detailed analysis of the impact of climate change on global agriculture over the past decade.\",\n",
    "    \"How do I do that?\",\n",
    "    \"Can you explain the process of photosynthesis in detail?\",\n",
    "    \"Fix it for me.\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    result = predict_ambiguity(prompt, model, tokenizer, DEVICE)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Is ambiguous: {result['is_ambiguous']} (confidence: {result['confidence']:.2f})\")\n",
    "    if result['is_ambiguous']:\n",
    "        print(f\"Ambiguity types: {', '.join(result['ambiguity_types'])}\")\n",
    "        print(f\"Clarifying question: {result['clarifying_question']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

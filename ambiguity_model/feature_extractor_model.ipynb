{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/devshah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/devshah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/devshah/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/devshah/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        22\n",
      "           1       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n",
      "Model saved to model.pkl\n",
      "\n",
      "Query: 'What is the capital of France?'\n",
      "Prediction: Clear\n",
      "Confidence: 100.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: capital\n",
      "  2. Incomplete Information: Query appears too short and may lack sufficient context\n",
      "\n",
      "Query: 'How do I cook pasta properly?'\n",
      "Prediction: Ambiguous\n",
      "Confidence: 82.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: cook\n",
      "  2. Incomplete Information: Query appears too short and may lack sufficient context\n",
      "\n",
      "Query: 'Why isn't it working?'\n",
      "Prediction: Ambiguous\n",
      "Confidence: 100.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: working\n",
      "  2. Incomplete Information: Query appears too short and may lack sufficient context\n",
      "\n",
      "Query: 'Can you help me with this thing?'\n",
      "Prediction: Ambiguous\n",
      "Confidence: 99.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: help, thing\n",
      "\n",
      "Query: 'What do they mean by that?'\n",
      "Prediction: Ambiguous\n",
      "Confidence: 98.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: mean\n",
      "  2. Incomplete Information: Query appears too short and may lack sufficient context\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import datetime\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class AmbiguityDetector:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def extract_features(self, texts):\n",
    "        \"\"\"Extract features related to ambiguity from text.\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for text in texts:\n",
    "\n",
    "            doc = nlp(text)\n",
    "            \n",
    "\n",
    "            text_features = {}\n",
    "            \n",
    "            # 1. Lexical ambiguity \n",
    "            polysemy_count = 0\n",
    "            content_words = 0\n",
    "            \n",
    "            for token in doc:\n",
    "                if token.is_alpha and not token.is_stop and token.pos_ in ['NOUN', 'VERB', 'ADJ']:\n",
    "                    content_words += 1\n",
    "                    synsets = wordnet.synsets(token.text)\n",
    "                    if len(synsets) > 1:\n",
    "                        polysemy_count += 1\n",
    "            \n",
    "            text_features['polysemy_ratio'] = polysemy_count / max(1, content_words)\n",
    "            \n",
    "            # 2. Referential ambiguity \n",
    "            pronouns = [token for token in doc if token.pos_ == 'PRON']\n",
    "            nouns = [token for token in doc if token.pos_ in ['NOUN', 'PROPN']]\n",
    "            text_features['pronoun_count'] = len(pronouns)\n",
    "            text_features['pronoun_noun_ratio'] = len(pronouns) / max(1, len(nouns))\n",
    "            \n",
    "            # 3. Vague quantifiers\n",
    "            vague_quantifiers = ['some', 'many', 'few', 'several', 'various', 'numerous', 'lot', 'lots']\n",
    "            vague_count = sum(1 for token in doc if token.text.lower() in vague_quantifiers)\n",
    "            text_features['vague_quantifier_count'] = vague_count\n",
    "            \n",
    "            # 4. Question words without specific context\n",
    "            question_words = ['who', 'what', 'where', 'when', 'why', 'how']\n",
    "            question_word_count = sum(1 for token in doc if token.text.lower() in question_words)\n",
    "            text_features['question_word_count'] = question_word_count\n",
    "            \n",
    "            # 5. Sentence complexity measures\n",
    "            text_features['avg_token_length'] = np.mean([len(token.text) for token in doc if token.is_alpha])\n",
    "            text_features['sentence_count'] = len(list(doc.sents))\n",
    "            text_features['avg_sentence_length'] = len(doc) / max(1, text_features['sentence_count'])\n",
    "            \n",
    "            # 6. Context specificity (measure of named entities)\n",
    "            named_entities = len(doc.ents)\n",
    "            text_features['named_entity_count'] = named_entities\n",
    "            \n",
    "            # 7. Conjunction count (potential for ambiguous scope)\n",
    "            conjunctions = sum(1 for token in doc if token.pos_ == 'CCONJ')\n",
    "            text_features['conjunction_count'] = conjunctions\n",
    "            \n",
    "            features.append(text_features)\n",
    "            \n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def train(self, texts, labels):\n",
    "        \"\"\"Train the ambiguity detection model.\"\"\"\n",
    "\n",
    "        X_tfidf = self.vectorizer.fit_transform(texts)\n",
    "        \n",
    "\n",
    "        X_custom = self.extract_features(texts)\n",
    "        \n",
    "\n",
    "        X_tfidf_array = X_tfidf.toarray()\n",
    "        X_combined = np.hstack([X_tfidf_array, X_custom.values])\n",
    "        \n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_combined, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "\n",
    "        y_pred = self.model.predict(X_val)\n",
    "        print(classification_report(y_val, y_pred))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Predict if a text is ambiguous.\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "            \n",
    "\n",
    "        X_tfidf = self.vectorizer.transform(text)\n",
    "        X_custom = self.extract_features(text)\n",
    "        \n",
    "\n",
    "        X_tfidf_array = X_tfidf.toarray()\n",
    "        X_combined = np.hstack([X_tfidf_array, X_custom.values])\n",
    "        \n",
    "\n",
    "        return self.model.predict(X_combined), self.model.predict_proba(X_combined)\n",
    "    \n",
    "    def analyze_ambiguity(self, text):\n",
    "        \"\"\"Analyze why a text might be ambiguous and return explanation.\"\"\"\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        ambiguity_types = []\n",
    "        explanations = []\n",
    "        \n",
    "        # Check for lexical ambiguity\n",
    "        polysemous_words = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha and not token.is_stop and token.pos_ in ['NOUN', 'VERB', 'ADJ']:\n",
    "                synsets = wordnet.synsets(token.text)\n",
    "                if len(synsets) > 2:  # Only consider highly ambiguous words\n",
    "                    polysemous_words.append(token.text)\n",
    "        \n",
    "        if polysemous_words:\n",
    "            ambiguity_types.append(\"Lexical Ambiguity\")\n",
    "            explanations.append(f\"Words with multiple meanings: {', '.join(polysemous_words[:3])}\")\n",
    "        \n",
    "        # Check for referential ambiguity\n",
    "        pronouns = [token.text for token in doc if token.pos_ == 'PRON']\n",
    "        if len(pronouns) > 2 and len([t for t in doc if t.pos_ in ['NOUN', 'PROPN']]) > 2:\n",
    "            ambiguity_types.append(\"Referential Ambiguity\")\n",
    "            explanations.append(f\"Multiple pronouns with potential unclear references: {', '.join(pronouns[:3])}\")\n",
    "        \n",
    "        # Check for vague quantifiers\n",
    "        vague_terms = ['some', 'many', 'few', 'several', 'various', 'numerous', 'lot', 'lots']\n",
    "        vague_found = [token.text for token in doc if token.text.lower() in vague_terms]\n",
    "        if vague_found:\n",
    "            ambiguity_types.append(\"Vague Quantifiers\")\n",
    "            explanations.append(f\"Vague quantity terms: {', '.join(vague_found)}\")\n",
    "        \n",
    "        # Check for missing information\n",
    "        question_words = ['who', 'what', 'where', 'when', 'why', 'how']\n",
    "        question_word_found = [token.text for token in doc if token.text.lower() in question_words]\n",
    "        if question_word_found and len(doc) < 10:\n",
    "            ambiguity_types.append(\"Incomplete Information\")\n",
    "            explanations.append(\"Query appears too short and may lack sufficient context\")\n",
    "        \n",
    "        return {\n",
    "            \"ambiguity_types\": ambiguity_types,\n",
    "            \"explanations\": explanations,\n",
    "            \"overall_assessment\": \"Ambiguous\" if ambiguity_types else \"Clear\"\n",
    "        }\n",
    "    \n",
    "    def save_model(self, filepath=None):\n",
    "        \"\"\"\n",
    "        Save the trained model to disk.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str, optional): Path to save the model. If None, generates a timestamped filename.\n",
    "        \n",
    "        Returns:\n",
    "            str: Path where the model was saved\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"No trained model to save. Train the model first.\")\n",
    "\n",
    "        if filepath is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = f\"ambiguity_detector_{timestamp}.pkl\"\n",
    "        \n",
    "\n",
    "        directory = os.path.dirname(filepath)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "\n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'vectorizer': self.vectorizer,\n",
    "            'feature_names': self.feature_names\n",
    "        }\n",
    "\n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath):\n",
    "        \"\"\"\n",
    "        Load a trained model from disk.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to the saved model file\n",
    "            \n",
    "        Returns:\n",
    "            AmbiguityDetector: Loaded model instance\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Model file not found: {filepath}\")\n",
    "        \n",
    "\n",
    "        try:\n",
    "            model_data = joblib.load(filepath)\n",
    "\n",
    "            instance = cls()\n",
    "\n",
    "            instance.model = model_data['model']\n",
    "            instance.vectorizer = model_data['vectorizer']\n",
    "            instance.feature_names = model_data['feature_names']\n",
    "            \n",
    "            print(f\"Model loaded from {filepath}\")\n",
    "            return instance\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading model: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_training_data(n_samples=1000):\n",
    "\n",
    "    clear_queries = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"How do I convert a pandas DataFrame to a CSV file?\",\n",
    "        \"Show me the recipe for chocolate chip cookies.\",\n",
    "        \"Explain the process of photosynthesis in detail.\",\n",
    "        \"What was the significance of the Magna Carta?\",\n",
    "        \"Write a Python function to calculate Fibonacci numbers.\",\n",
    "        \"Summarize the plot of The Great Gatsby by F. Scott Fitzgerald.\",\n",
    "        \"List five exercises for strengthening core muscles.\",\n",
    "        \"What temperature should I cook chicken to ensure it's safe?\",\n",
    "        \"Explain the difference between HTTP and HTTPS protocols.\"\n",
    "    ]\n",
    "    \n",
    "    ambiguous_queries = [\n",
    "        \"Why is it doing that?\",\n",
    "        \"Can you help me with this?\",\n",
    "        \"Is it good?\",\n",
    "        \"What do they mean?\",\n",
    "        \"How does it work?\",\n",
    "        \"Can you tell me more about that topic?\",\n",
    "        \"Why isn't it working?\",\n",
    "        \"What's the best one?\",\n",
    "        \"How do I fix it?\",\n",
    "        \"When should I use it?\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    np.random.seed(42)\n",
    "    clear_expanded = []\n",
    "    for query in clear_queries:\n",
    "        clear_expanded.append(query)\n",
    "        words = query.split()\n",
    "        for _ in range(9):  # Create 9 variations of each template\n",
    "\n",
    "            if len(words) > 5:\n",
    "                modified = ' '.join(words[:len(words)-np.random.randint(1, 3)]) + ' ' + ' '.join(words[-np.random.randint(1, 3):])\n",
    "                clear_expanded.append(modified)\n",
    "            else:\n",
    "                clear_expanded.append(query)\n",
    "    \n",
    "    ambiguous_expanded = []\n",
    "    for query in ambiguous_queries:\n",
    "        ambiguous_expanded.append(query)\n",
    "        for _ in range(9):  # Create 9 variations of each template\n",
    "            # Might add filler words but keep ambiguous\n",
    "            if np.random.random() > 0.5:\n",
    "                fillers = ['um', 'like', 'so', 'you know', 'I mean']\n",
    "                ambiguous_expanded.append(np.random.choice(fillers) + ' ' + query)\n",
    "            else:\n",
    "                ambiguous_expanded.append(query)\n",
    "    \n",
    "\n",
    "    all_texts = clear_expanded + ambiguous_expanded\n",
    "    labels = [0] * len(clear_expanded) + [1] * len(ambiguous_expanded)\n",
    "    \n",
    "\n",
    "    indices = np.arange(len(all_texts))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return [all_texts[i] for i in indices[:n_samples]], [labels[i] for i in indices[:n_samples]]\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    texts, labels = generate_training_data(500)\n",
    "    \n",
    "    # Train ambiguity detector\n",
    "    detector = AmbiguityDetector()\n",
    "    detector.train(texts, labels)\n",
    "\n",
    "    detector.save_model(\"model.pkl\")\n",
    "    \n",
    "\n",
    "    test_queries = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"How do I cook pasta properly?\",\n",
    "        \"Why isn't it working?\",\n",
    "        \"Can you help me with this thing?\",\n",
    "        \"What do they mean by that?\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        prediction, probabilities = detector.predict(query)\n",
    "        ambiguity_analysis = detector.analyze_ambiguity(query)\n",
    "        \n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"Prediction: {'Ambiguous' if prediction[0] == 1 else 'Clear'}\")\n",
    "        print(f\"Confidence: {max(probabilities[0]) * 100:.2f}%\")\n",
    "        \n",
    "        if ambiguity_analysis['ambiguity_types']:\n",
    "            print(\"Ambiguity Analysis:\")\n",
    "            for i, (ambiguity_type, explanation) in enumerate(zip(\n",
    "                ambiguity_analysis['ambiguity_types'], \n",
    "                ambiguity_analysis['explanations']\n",
    "            )):\n",
    "                print(f\"  {i+1}. {ambiguity_type}: {explanation}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model.pkl\n",
      "\n",
      "Query: 'im confused'\n",
      "Prediction: Ambiguous\n",
      "Confidence: 91.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: confused\n"
     ]
    }
   ],
   "source": [
    "query = 'im confused'\n",
    "\n",
    "detector = AmbiguityDetector.load_model(\"model.pkl\")\n",
    "\n",
    "prediction, probabilities = detector.predict(query)\n",
    "ambiguity_analysis = detector.analyze_ambiguity(query)\n",
    "        \n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"Prediction: {'Ambiguous' if prediction[0] == 1 else 'Clear'}\")\n",
    "print(f\"Confidence: {max(probabilities[0]) * 100:.2f}%\")\n",
    "        \n",
    "if ambiguity_analysis['ambiguity_types']:\n",
    "    print(\"Ambiguity Analysis:\")\n",
    "    for i, (ambiguity_type, explanation) in enumerate(zip(\n",
    "        ambiguity_analysis['ambiguity_types'], \n",
    "        ambiguity_analysis['explanations']\n",
    "            )):\n",
    "        print(f\"  {i+1}. {ambiguity_type}: {explanation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        38\n",
      "           1       1.00      1.00      1.00        44\n",
      "\n",
      "    accuracy                           1.00        82\n",
      "   macro avg       1.00      1.00      1.00        82\n",
      "weighted avg       1.00      1.00      1.00        82\n",
      "\n",
      "Model saved to model.pkl\n",
      "\n",
      "Query: 'How did Mark Zuckerberg become rich?'\n",
      "Prediction: Clear\n",
      "Confidence: 65.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: rich\n",
      "  2. Incomplete Information: Query appears too short and may lack sufficient context\n",
      "\n",
      "Query: 'How long will a flight take?'\n",
      "Prediction: Ambiguous\n",
      "Confidence: 69.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: flight\n",
      "  2. Incomplete Information: Query appears too short and may lack sufficient context\n",
      "\n",
      "Query: 'What's the best field?'\n",
      "Prediction: Ambiguous\n",
      "Confidence: 99.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: best, field\n",
      "  2. Incomplete Information: Query appears too short and may lack sufficient context\n",
      "\n",
      "Query: 'When will this end?'\n",
      "Prediction: Ambiguous\n",
      "Confidence: 85.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: end\n",
      "  2. Incomplete Information: Query appears too short and may lack sufficient context\n",
      "\n",
      "Query: 'What's the most effective calves workout?'\n",
      "Prediction: Clear\n",
      "Confidence: 56.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: effective, calves\n",
      "  2. Incomplete Information: Query appears too short and may lack sufficient context\n",
      "\n",
      "Query: 'What is something useful I can buy with $5'\n",
      "Prediction: Ambiguous\n",
      "Confidence: 71.00%\n",
      "Ambiguity Analysis:\n",
      "  1. Lexical Ambiguity: Words with multiple meanings: buy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import datetime\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class AmbiguityDetector:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        self.feature_names = None\n",
    "\n",
    "    def extract_features(self, texts):\n",
    "        \"\"\"Extract features related to ambiguity from text.\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for text in texts:\n",
    "\n",
    "            doc = nlp(text)\n",
    "            \n",
    "\n",
    "            text_features = {}\n",
    "            \n",
    "            # 1. Lexical ambiguity \n",
    "            polysemy_count = 0\n",
    "            content_words = 0\n",
    "            \n",
    "            for token in doc:\n",
    "                if token.is_alpha and not token.is_stop and token.pos_ in ['NOUN', 'VERB', 'ADJ']:\n",
    "                    content_words += 1\n",
    "                    synsets = wordnet.synsets(token.text)\n",
    "                    if len(synsets) > 1:\n",
    "                        polysemy_count += 1\n",
    "            \n",
    "            text_features['polysemy_ratio'] = polysemy_count / max(1, content_words)\n",
    "            \n",
    "            # 2. Referential ambiguity \n",
    "            pronouns = [token for token in doc if token.pos_ == 'PRON']\n",
    "            nouns = [token for token in doc if token.pos_ in ['NOUN', 'PROPN']]\n",
    "            text_features['pronoun_count'] = len(pronouns)\n",
    "            text_features['pronoun_noun_ratio'] = len(pronouns) / max(1, len(nouns))\n",
    "            \n",
    "            # 3. Vague quantifiers\n",
    "            vague_quantifiers = ['some', 'many', 'few', 'several', 'various', 'numerous', 'lot', 'lots']\n",
    "            vague_count = sum(1 for token in doc if token.text.lower() in vague_quantifiers)\n",
    "            text_features['vague_quantifier_count'] = vague_count\n",
    "            \n",
    "            # 4. Question words without specific context\n",
    "            question_words = ['who', 'what', 'where', 'when', 'why', 'how']\n",
    "            question_word_count = sum(1 for token in doc if token.text.lower() in question_words)\n",
    "            text_features['question_word_count'] = question_word_count\n",
    "            \n",
    "            # 5. Sentence complexity measures\n",
    "            text_features['avg_token_length'] = np.mean([len(token.text) for token in doc if token.is_alpha])\n",
    "            text_features['sentence_count'] = len(list(doc.sents))\n",
    "            text_features['avg_sentence_length'] = len(doc) / max(1, text_features['sentence_count'])\n",
    "            \n",
    "            # 6. Context specificity (measure of named entities)\n",
    "            named_entities = len(doc.ents)\n",
    "            text_features['named_entity_count'] = named_entities\n",
    "            \n",
    "            # 7. Conjunction count (potential for ambiguous scope)\n",
    "            conjunctions = sum(1 for token in doc if token.pos_ == 'CCONJ')\n",
    "            text_features['conjunction_count'] = conjunctions\n",
    "\n",
    "            text_features['syntatic_ambiguity'] = self.detect_syntactic_ambiguity(text)\n",
    "            text_features['modal_verb_count'] = self.detect_modal_verbs(text)\n",
    "            \n",
    "            features.append(text_features)\n",
    "            \n",
    "        return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "    def detect_syntactic_ambiguity(self, text):\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                if token.dep_ in ('prep', 'pobj') and token.head.pos_ == 'VERB':\n",
    "                    return True  # Potential syntactic ambiguity\n",
    "        return False\n",
    "        \n",
    "    def detect_modal_verbs(self, text):\n",
    "        modal_verbs = {'can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would', 'must'}\n",
    "        doc = nlp(text)\n",
    "        return sum(1 for token in doc if token.text.lower() in modal_verbs)\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self, texts, labels):\n",
    "        \"\"\"Train the ambiguity detection model.\"\"\"\n",
    "\n",
    "        X_tfidf = self.vectorizer.fit_transform(texts)\n",
    "        \n",
    "\n",
    "        X_custom = self.extract_features(texts)\n",
    "        \n",
    "\n",
    "        X_tfidf_array = X_tfidf.toarray()\n",
    "        X_combined = np.hstack([X_tfidf_array, X_custom.values])\n",
    "        \n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_combined, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "\n",
    "        y_pred = self.model.predict(X_val)\n",
    "        print(classification_report(y_val, y_pred))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Predict if a text is ambiguous.\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "            \n",
    "\n",
    "        X_tfidf = self.vectorizer.transform(text)\n",
    "        X_custom = self.extract_features(text)\n",
    "        \n",
    "\n",
    "        X_tfidf_array = X_tfidf.toarray()\n",
    "        X_combined = np.hstack([X_tfidf_array, X_custom.values])\n",
    "        \n",
    "\n",
    "        return self.model.predict(X_combined), self.model.predict_proba(X_combined)\n",
    "    \n",
    "    def analyze_ambiguity(self, text):\n",
    "        \"\"\"Analyze why a text might be ambiguous and return explanation.\"\"\"\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        ambiguity_types = []\n",
    "        explanations = []\n",
    "        \n",
    "        # Check for lexical ambiguity\n",
    "        polysemous_words = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha and not token.is_stop and token.pos_ in ['NOUN', 'VERB', 'ADJ']:\n",
    "                synsets = wordnet.synsets(token.text)\n",
    "                \n",
    "                #---------------- added synonym checking\n",
    "\n",
    "                synonyms = set()\n",
    "                for syn in synsets:\n",
    "                    synonyms.update(syn.lemma_names())\n",
    "\n",
    "                #-------------------------\n",
    "                \n",
    "                \n",
    "                if len(synsets) > 2 and len(synonyms) > 4:  # Only consider highly ambiguous words\n",
    "                    polysemous_words.append(token.text)\n",
    "        \n",
    "        if polysemous_words:\n",
    "            ambiguity_types.append(\"Lexical Ambiguity\")\n",
    "            explanations.append(f\"Words with multiple meanings: {', '.join(polysemous_words[:3])}\")\n",
    "        \n",
    "        # Check for referential ambiguity\n",
    "        pronouns = [token.text for token in doc if token.pos_ == 'PRON']\n",
    "        if len(pronouns) > 2 and len([t for t in doc if t.pos_ in ['NOUN', 'PROPN']]) > 2:\n",
    "            ambiguity_types.append(\"Referential Ambiguity\")\n",
    "            explanations.append(f\"Multiple pronouns with potential unclear references: {', '.join(pronouns[:3])}\")\n",
    "        \n",
    "        # Check for vague quantifiers\n",
    "        vague_terms = ['some', 'many', 'few', 'several', 'various', 'numerous', 'lot', 'lots', 'tons', 'a bunch', 'plenty'\n",
    "                       'a couple', 'a handful', 'loads', 'countless', 'a great deal', 'a good amount']\n",
    "        vague_found = [token.text for token in doc if token.text.lower() in vague_terms]\n",
    "        if vague_found:\n",
    "            ambiguity_types.append(\"Vague Quantifiers\")\n",
    "            explanations.append(f\"Vague quantity terms: {', '.join(vague_found)}\")\n",
    "        \n",
    "        # Check for missing information\n",
    "        question_words = ['who', 'what', 'where', 'when', 'why', 'how', 'which', \"whose\", \"whom\", \"whenever\", \n",
    "                          \"wherever\", \"whichever\", \"whatever\", \"however\"]\n",
    "        question_word_found = [token.text for token in doc if token.text.lower() in question_words]\n",
    "        if question_word_found and len(doc) < 10:\n",
    "            ambiguity_types.append(\"Incomplete Information\")\n",
    "            explanations.append(\"Query appears too short and may lack sufficient context\")\n",
    "        \n",
    "        return {\n",
    "            \"ambiguity_types\": ambiguity_types,\n",
    "            \"explanations\": explanations,\n",
    "            \"overall_assessment\": \"Ambiguous\" if ambiguity_types else \"Clear\"\n",
    "        }\n",
    "    \n",
    "    def save_model(self, filepath=None):\n",
    "        \"\"\"\n",
    "        Save the trained model to disk.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str, optional): Path to save the model. If None, generates a timestamped filename.\n",
    "        \n",
    "        Returns:\n",
    "            str: Path where the model was saved\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"No trained model to save. Train the model first.\")\n",
    "\n",
    "        if filepath is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filepath = f\"ambiguity_detector_{timestamp}.pkl\"\n",
    "        \n",
    "\n",
    "        directory = os.path.dirname(filepath)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "\n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'vectorizer': self.vectorizer,\n",
    "            'feature_names': self.feature_names\n",
    "        }\n",
    "\n",
    "        joblib.dump(model_data, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath):\n",
    "        \"\"\"\n",
    "        Load a trained model from disk.\n",
    "        \n",
    "        Args:\n",
    "            filepath (str): Path to the saved model file\n",
    "            \n",
    "        Returns:\n",
    "            AmbiguityDetector: Loaded model instance\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            raise FileNotFoundError(f\"Model file not found: {filepath}\")\n",
    "        \n",
    "\n",
    "        try:\n",
    "            model_data = joblib.load(filepath)\n",
    "\n",
    "            instance = cls()\n",
    "\n",
    "            instance.model = model_data['model']\n",
    "            instance.vectorizer = model_data['vectorizer']\n",
    "            instance.feature_names = model_data['feature_names']\n",
    "            \n",
    "            print(f\"Model loaded from {filepath}\")\n",
    "            return instance\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading model: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_training_data(n_samples=1000):\n",
    "\n",
    "    clear_queries = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"How do I convert a pandas DataFrame to a CSV file?\",\n",
    "        \"Show me the recipe for chocolate chip cookies.\",\n",
    "        \"Explain the process of photosynthesis in detail.\",\n",
    "        \"What was the significance of the Magna Carta?\",\n",
    "        \"Write a Python function to calculate Fibonacci numbers.\",\n",
    "        \"Summarize the plot of The Great Gatsby by F. Scott Fitzgerald.\",\n",
    "        \"List five exercises for strengthening core muscles.\",\n",
    "        \"What temperature should I cook chicken to ensure it's safe?\",\n",
    "        \"Explain the difference between HTTP and HTTPS protocols.\", ## I added more below\n",
    "        \"Which is healthier: running or swimming?\",\n",
    "        \"What are the steps to apply for a US visa?\",\n",
    "        \"What lessons can businesses learn from the 2008 financial crisis?\",\n",
    "        \"What would happen if the Internet shut down for a week?\",\n",
    "        \"How old was Alexander the Great when he came into power?\",\n",
    "        \"When did Germany become a country?\"\n",
    "        \"How do I change a tire when I have no car jack?\",\n",
    "        \"What are the most effective study tactics for students with ADHD?\",\n",
    "        \"What meals are good for muscle recovery?\"\n",
    "    ]\n",
    "    \n",
    "    ambiguous_queries = [\n",
    "        \"Why is it doing that?\",\n",
    "        \"Can you help me with this?\",\n",
    "        \"Is it good?\",\n",
    "        \"What do they mean?\",\n",
    "        \"How does it work?\",\n",
    "        \"Can you tell me more about that topic?\",\n",
    "        \"Why isn't it working?\",\n",
    "        \"What's the best one?\",\n",
    "        \"How do I fix it?\",\n",
    "        \"When should I use it?\", ## added more below\n",
    "        \"What are the best exercises for legs?\",\n",
    "        \"What is the best workout meal?\",\n",
    "        \"What is the best studying technique?\",\n",
    "        \"What car should I buy?\",\n",
    "        \"When will it be over?\",\n",
    "        \"Who made the wall?\",\n",
    "        \"Why calculators?\",\n",
    "        \"How is Antartica?\",\n",
    "        \"When Muhammad Ali fought him, did he win?\",\n",
    "        \"Should I do it now or later?\",\n",
    "        \"Is this one better?\",\n",
    "        \"Tell me more about that.\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    np.random.seed(42)\n",
    "    clear_expanded = []\n",
    "    for query in clear_queries:\n",
    "        clear_expanded.append(query)\n",
    "        words = query.split()\n",
    "        for _ in range(9):  # Create 9 variations of each template\n",
    "\n",
    "            if len(words) > 5:\n",
    "                modified = ' '.join(words[:len(words)-np.random.randint(1, 3)]) + ' ' + ' '.join(words[-np.random.randint(1, 3):])\n",
    "                clear_expanded.append(modified)\n",
    "            else:\n",
    "                clear_expanded.append(query)\n",
    "    \n",
    "    ambiguous_expanded = []\n",
    "    for query in ambiguous_queries:\n",
    "        ambiguous_expanded.append(query)\n",
    "        \n",
    "        #adding vague endings\n",
    "        vague_endings = [\"or something\", \"maybe\", \"I guess\", \"I think\", \"...\"]\n",
    "        fillers = ['um', 'like', 'so', 'you know', 'I mean']\n",
    "\n",
    "        for _ in range(9):  # Create 9 variations of each template\n",
    "            # Might add filler words but keep ambiguous\n",
    "            rand_num = np.random.random()\n",
    "            if rand_num > 0.6:\n",
    "                ambiguous_expanded.append(np.random.choice(fillers) + \" \" + query)\n",
    "            elif rand_num > 0.3:\n",
    "                ambiguous_expanded.append(query + \" \" + np.random.choice(vague_endings))\n",
    "            else: \n",
    "                truncated = \" \".join(query.split()[:-1])\n",
    "                ambiguous_expanded.append(truncated if truncated else query)\n",
    "    \n",
    "    all_texts = clear_expanded + ambiguous_expanded\n",
    "    labels = [0] * len(clear_expanded) + [1] * len(ambiguous_expanded)\n",
    "    \n",
    "\n",
    "    indices = np.arange(len(all_texts))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    return [all_texts[i] for i in indices[:n_samples]], [labels[i] for i in indices[:n_samples]]\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    texts, labels = generate_training_data(500)\n",
    "    \n",
    "    # Train ambiguity detector\n",
    "    detector = AmbiguityDetector()\n",
    "    detector.train(texts, labels)\n",
    "\n",
    "    detector.save_model(\"model.pkl\")\n",
    "    \n",
    "\n",
    "    test_queries = [\n",
    "        \"How did Mark Zuckerberg become rich?\", # clear\n",
    "        \"How long will a flight take?\", # ambiguous\n",
    "        \"What's the best field?\", # ambiguous\n",
    "        \"When will this end?\", # ambiguous\n",
    "        \"What's the most effective calves workout?\", # clear\n",
    "        \"What is something useful I can buy with $5\" # clear\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        prediction, probabilities = detector.predict(query)\n",
    "        ambiguity_analysis = detector.analyze_ambiguity(query)\n",
    "        \n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"Prediction: {'Ambiguous' if prediction[0] == 1 else 'Clear'}\")\n",
    "        print(f\"Confidence: {max(probabilities[0]) * 100:.2f}%\")\n",
    "        \n",
    "        if ambiguity_analysis['ambiguity_types']:\n",
    "            print(\"Ambiguity Analysis:\")\n",
    "            for i, (ambiguity_type, explanation) in enumerate(zip(\n",
    "                ambiguity_analysis['ambiguity_types'], \n",
    "                ambiguity_analysis['explanations']\n",
    "            )):\n",
    "                print(f\"  {i+1}. {ambiguity_type}: {explanation}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
